import os
import logging
from typing import List, Optional
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
import chromadb
from chromadb.config import Settings
import time

logger = logging.getLogger(__name__)

class OpenAIEmbeddingFunction:
    def __init__(self, openai_api_key: str, model: str = "text-embedding-ada-002"):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=openai_api_key,
            model=model
        )
    
    def __call__(self, input: List[str]) -> List[List[float]]:
        return self.embeddings.embed_documents(input)

class VectorStore:
    def __init__(self, openai_api_key: str, collection_name: str = "insurance_docs"):
        self.openai_api_key = openai_api_key
        self.collection_name = collection_name
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path="./vector_store",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
        self.embedding_function = OpenAIEmbeddingFunction(openai_api_key)
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        try:
            self.collection = self.client.get_collection(
                name=collection_name,
                embedding_function=self.embedding_function
            )
            logger.info(f"âœ… Existing collection '{collection_name}' loaded")
        except Exception:
            self.collection = self.client.create_collection(
                name=collection_name,
                embedding_function=self.embedding_function
            )
            logger.info(f"âœ… New collection '{collection_name}' created")
    
    def add_documents_batch(self, documents: List[Document], batch_size: int = 100, max_retries: int = 3):
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¬¸ì„œ ì¶”ê°€"""
        total_docs = len(documents)
        logger.info(f"ğŸ”„ Adding {total_docs} documents in batches of {batch_size}")
        
        for i in range(0, total_docs, batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_docs + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ Processing batch {batch_num}/{total_batches} ({len(batch)} documents)")
            
            # ì¬ì‹œë„ ë¡œì§
            for attempt in range(max_retries):
                try:
                    texts = [doc.page_content for doc in batch]
                    metadatas = [doc.metadata for doc in batch]
                    ids = [f"doc_{i + j}" for j in range(len(batch))]
                    
                    self.collection.add(
                        documents=texts,
                        metadatas=metadatas,
                        ids=ids
                    )
                    
                    logger.info(f"âœ… Batch {batch_num} completed successfully")
                    break
                    
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        logger.warning(f"âš ï¸ Batch {batch_num} failed (attempt {attempt + 1}), retrying in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"âŒ Batch {batch_num} failed after {max_retries} attempts: {e}")
                        raise
            
            # API ë ˆì´íŠ¸ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
            if i + batch_size < total_docs:
                time.sleep(1)
        
        logger.info(f"ğŸ‰ All {total_docs} documents added successfully!")
    
    def add_documents(self, documents: List[Document]):
        """ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„± ìœ ì§€)"""
        self.add_documents_batch(documents)
    
    def similarity_search(self, query: str, k: int = 5) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=k
            )
            
            documents = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {}
                    documents.append(Document(page_content=doc, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"âŒ Similarity search failed: {e}")
            return []
    
    def get_collection_info(self) -> dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜"""
        try:
            count = self.collection.count()
            return {
                "name": self.collection_name,
                "count": count,
                "status": "ready" if count > 0 else "empty"
            }
        except Exception as e:
            logger.error(f"âŒ Failed to get collection info: {e}")
            return {
                "name": self.collection_name,
                "count": 0,
                "status": "error"
            } 